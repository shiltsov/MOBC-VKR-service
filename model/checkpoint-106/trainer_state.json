{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 106,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.018867924528301886,
      "grad_norm": 31.82872200012207,
      "learning_rate": 5e-05,
      "loss": 14.2248,
      "step": 1
    },
    {
      "epoch": 0.03773584905660377,
      "grad_norm": 5.056722640991211,
      "learning_rate": 0.0001,
      "loss": 8.2002,
      "step": 2
    },
    {
      "epoch": 0.05660377358490566,
      "grad_norm": 87.14688873291016,
      "learning_rate": 0.00015,
      "loss": 8.5361,
      "step": 3
    },
    {
      "epoch": 0.07547169811320754,
      "grad_norm": 43.632144927978516,
      "learning_rate": 0.0002,
      "loss": 9.8184,
      "step": 4
    },
    {
      "epoch": 0.09433962264150944,
      "grad_norm": 10.375312805175781,
      "learning_rate": 0.00025,
      "loss": 13.0669,
      "step": 5
    },
    {
      "epoch": 0.11320754716981132,
      "grad_norm": 4.088149070739746,
      "learning_rate": 0.0003,
      "loss": 11.8838,
      "step": 6
    },
    {
      "epoch": 0.1320754716981132,
      "grad_norm": 172.22251892089844,
      "learning_rate": 0.00035,
      "loss": 10.4242,
      "step": 7
    },
    {
      "epoch": 0.1509433962264151,
      "grad_norm": 8.041447639465332,
      "learning_rate": 0.0004,
      "loss": 8.0315,
      "step": 8
    },
    {
      "epoch": 0.16981132075471697,
      "grad_norm": 13.666894912719727,
      "learning_rate": 0.00045000000000000004,
      "loss": 8.715,
      "step": 9
    },
    {
      "epoch": 0.18867924528301888,
      "grad_norm": 11.064544677734375,
      "learning_rate": 0.0005,
      "loss": 11.9003,
      "step": 10
    },
    {
      "epoch": 0.20754716981132076,
      "grad_norm": 12.546219825744629,
      "learning_rate": 0.0004998661468690914,
      "loss": 7.4734,
      "step": 11
    },
    {
      "epoch": 0.22641509433962265,
      "grad_norm": 4.081808090209961,
      "learning_rate": 0.0004994647308096509,
      "loss": 7.2727,
      "step": 12
    },
    {
      "epoch": 0.24528301886792453,
      "grad_norm": 42.40277862548828,
      "learning_rate": 0.0004987961816680492,
      "loss": 15.037,
      "step": 13
    },
    {
      "epoch": 0.2641509433962264,
      "grad_norm": 27.036876678466797,
      "learning_rate": 0.0004978612153434526,
      "loss": 10.5728,
      "step": 14
    },
    {
      "epoch": 0.2830188679245283,
      "grad_norm": 4.797910690307617,
      "learning_rate": 0.0004966608330212198,
      "loss": 6.2764,
      "step": 15
    },
    {
      "epoch": 0.3018867924528302,
      "grad_norm": 4.183885097503662,
      "learning_rate": 0.0004951963201008077,
      "loss": 6.3307,
      "step": 16
    },
    {
      "epoch": 0.32075471698113206,
      "grad_norm": 38.73165512084961,
      "learning_rate": 0.0004934692448193334,
      "loss": 6.7738,
      "step": 17
    },
    {
      "epoch": 0.33962264150943394,
      "grad_norm": 9.578958511352539,
      "learning_rate": 0.0004914814565722671,
      "loss": 5.8766,
      "step": 18
    },
    {
      "epoch": 0.3584905660377358,
      "grad_norm": 64.89820098876953,
      "learning_rate": 0.0004892350839330522,
      "loss": 8.5956,
      "step": 19
    },
    {
      "epoch": 0.37735849056603776,
      "grad_norm": 75.94768524169922,
      "learning_rate": 0.00048673253237377644,
      "loss": 6.9864,
      "step": 20
    },
    {
      "epoch": 0.39622641509433965,
      "grad_norm": 9.462235450744629,
      "learning_rate": 0.00048397648168933144,
      "loss": 4.1743,
      "step": 21
    },
    {
      "epoch": 0.41509433962264153,
      "grad_norm": 7.163152694702148,
      "learning_rate": 0.0004809698831278217,
      "loss": 3.4247,
      "step": 22
    },
    {
      "epoch": 0.4339622641509434,
      "grad_norm": 12.601997375488281,
      "learning_rate": 0.000477715956230294,
      "loss": 2.9423,
      "step": 23
    },
    {
      "epoch": 0.4528301886792453,
      "grad_norm": 18.87416648864746,
      "learning_rate": 0.0004742181853831721,
      "loss": 3.3941,
      "step": 24
    },
    {
      "epoch": 0.4716981132075472,
      "grad_norm": 7.660436630249023,
      "learning_rate": 0.00047048031608708875,
      "loss": 1.5485,
      "step": 25
    },
    {
      "epoch": 0.49056603773584906,
      "grad_norm": 3.4884655475616455,
      "learning_rate": 0.00046650635094610973,
      "loss": 1.1321,
      "step": 26
    },
    {
      "epoch": 0.5094339622641509,
      "grad_norm": 5.091476917266846,
      "learning_rate": 0.00046230054538164475,
      "loss": 1.2862,
      "step": 27
    },
    {
      "epoch": 0.5283018867924528,
      "grad_norm": 1.8368569612503052,
      "learning_rate": 0.00045786740307563633,
      "loss": 1.1993,
      "step": 28
    },
    {
      "epoch": 0.5471698113207547,
      "grad_norm": 1.2478761672973633,
      "learning_rate": 0.0004532116711479038,
      "loss": 0.895,
      "step": 29
    },
    {
      "epoch": 0.5660377358490566,
      "grad_norm": 2.394458532333374,
      "learning_rate": 0.0004483383350728088,
      "loss": 1.4123,
      "step": 30
    },
    {
      "epoch": 0.5849056603773585,
      "grad_norm": 1.9994268417358398,
      "learning_rate": 0.0004432526133406842,
      "loss": 1.15,
      "step": 31
    },
    {
      "epoch": 0.6037735849056604,
      "grad_norm": 1.8333977460861206,
      "learning_rate": 0.00043795995186974435,
      "loss": 1.1474,
      "step": 32
    },
    {
      "epoch": 0.6226415094339622,
      "grad_norm": 2.2463626861572266,
      "learning_rate": 0.0004324660181744589,
      "loss": 1.0185,
      "step": 33
    },
    {
      "epoch": 0.6415094339622641,
      "grad_norm": 4.296903610229492,
      "learning_rate": 0.00042677669529663686,
      "loss": 0.9496,
      "step": 34
    },
    {
      "epoch": 0.660377358490566,
      "grad_norm": 3.955024003982544,
      "learning_rate": 0.0004208980755057178,
      "loss": 0.9765,
      "step": 35
    },
    {
      "epoch": 0.6792452830188679,
      "grad_norm": 26.62883758544922,
      "learning_rate": 0.0004148364537750172,
      "loss": 1.6269,
      "step": 36
    },
    {
      "epoch": 0.6981132075471698,
      "grad_norm": 3.6294286251068115,
      "learning_rate": 0.0004085983210409114,
      "loss": 0.8705,
      "step": 37
    },
    {
      "epoch": 0.7169811320754716,
      "grad_norm": 2.8308253288269043,
      "learning_rate": 0.0004021903572521802,
      "loss": 0.942,
      "step": 38
    },
    {
      "epoch": 0.7358490566037735,
      "grad_norm": 3.3197314739227295,
      "learning_rate": 0.00039561942421695057,
      "loss": 0.9459,
      "step": 39
    },
    {
      "epoch": 0.7547169811320755,
      "grad_norm": 0.9447576999664307,
      "learning_rate": 0.00038889255825490053,
      "loss": 0.8632,
      "step": 40
    },
    {
      "epoch": 0.7735849056603774,
      "grad_norm": 1.076642632484436,
      "learning_rate": 0.000382016962662592,
      "loss": 0.6614,
      "step": 41
    },
    {
      "epoch": 0.7924528301886793,
      "grad_norm": 0.7594099044799805,
      "learning_rate": 0.000375,
      "loss": 0.6591,
      "step": 42
    },
    {
      "epoch": 0.8113207547169812,
      "grad_norm": 0.8193876147270203,
      "learning_rate": 0.0003678491842064995,
      "loss": 0.8782,
      "step": 43
    },
    {
      "epoch": 0.8301886792452831,
      "grad_norm": 0.9482760429382324,
      "learning_rate": 0.00036057217255475036,
      "loss": 0.7568,
      "step": 44
    },
    {
      "epoch": 0.8490566037735849,
      "grad_norm": 1.0346713066101074,
      "learning_rate": 0.00035317675745109866,
      "loss": 0.6125,
      "step": 45
    },
    {
      "epoch": 0.8679245283018868,
      "grad_norm": 1.4674056768417358,
      "learning_rate": 0.0003456708580912725,
      "loss": 0.6433,
      "step": 46
    },
    {
      "epoch": 0.8867924528301887,
      "grad_norm": 0.6369452476501465,
      "learning_rate": 0.0003380625119803084,
      "loss": 0.526,
      "step": 47
    },
    {
      "epoch": 0.9056603773584906,
      "grad_norm": 1.3893002271652222,
      "learning_rate": 0.0003303598663257904,
      "loss": 0.4955,
      "step": 48
    },
    {
      "epoch": 0.9245283018867925,
      "grad_norm": 1.6530897617340088,
      "learning_rate": 0.00032257116931361555,
      "loss": 0.5762,
      "step": 49
    },
    {
      "epoch": 0.9433962264150944,
      "grad_norm": 1.8203269243240356,
      "learning_rate": 0.00031470476127563017,
      "loss": 0.6193,
      "step": 50
    },
    {
      "epoch": 0.9622641509433962,
      "grad_norm": 0.9671713709831238,
      "learning_rate": 0.0003067690657585933,
      "loss": 0.7101,
      "step": 51
    },
    {
      "epoch": 0.9811320754716981,
      "grad_norm": 4.952826499938965,
      "learning_rate": 0.0002987725805040321,
      "loss": 0.7531,
      "step": 52
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.4475133419036865,
      "learning_rate": 0.0002907238683486472,
      "loss": 0.4253,
      "step": 53
    },
    {
      "epoch": 1.0188679245283019,
      "grad_norm": 0.9487017393112183,
      "learning_rate": 0.000282631548055013,
      "loss": 0.3938,
      "step": 54
    },
    {
      "epoch": 1.0377358490566038,
      "grad_norm": 0.725733757019043,
      "learning_rate": 0.0002745042850823902,
      "loss": 0.5091,
      "step": 55
    },
    {
      "epoch": 1.0566037735849056,
      "grad_norm": 1.5729857683181763,
      "learning_rate": 0.0002663507823075358,
      "loss": 0.6686,
      "step": 56
    },
    {
      "epoch": 1.0754716981132075,
      "grad_norm": 6.129166126251221,
      "learning_rate": 0.00025817977070544405,
      "loss": 0.4903,
      "step": 57
    },
    {
      "epoch": 1.0943396226415094,
      "grad_norm": 0.7117186188697815,
      "learning_rate": 0.00025,
      "loss": 0.517,
      "step": 58
    },
    {
      "epoch": 1.1132075471698113,
      "grad_norm": 0.4982987940311432,
      "learning_rate": 0.00024182022929455598,
      "loss": 0.5311,
      "step": 59
    },
    {
      "epoch": 1.1320754716981132,
      "grad_norm": 0.809506356716156,
      "learning_rate": 0.00023364921769246423,
      "loss": 0.5222,
      "step": 60
    },
    {
      "epoch": 1.150943396226415,
      "grad_norm": 0.5502213835716248,
      "learning_rate": 0.00022549571491760985,
      "loss": 0.4151,
      "step": 61
    },
    {
      "epoch": 1.169811320754717,
      "grad_norm": 0.7874225378036499,
      "learning_rate": 0.00021736845194498717,
      "loss": 0.4112,
      "step": 62
    },
    {
      "epoch": 1.1886792452830188,
      "grad_norm": 4.54761266708374,
      "learning_rate": 0.00020927613165135284,
      "loss": 0.674,
      "step": 63
    },
    {
      "epoch": 1.2075471698113207,
      "grad_norm": 0.7209662199020386,
      "learning_rate": 0.00020122741949596797,
      "loss": 0.5882,
      "step": 64
    },
    {
      "epoch": 1.2264150943396226,
      "grad_norm": 0.6692630052566528,
      "learning_rate": 0.00019323093424140672,
      "loss": 0.469,
      "step": 65
    },
    {
      "epoch": 1.2452830188679245,
      "grad_norm": 0.776801586151123,
      "learning_rate": 0.0001852952387243698,
      "loss": 0.5499,
      "step": 66
    },
    {
      "epoch": 1.2641509433962264,
      "grad_norm": 1.209871530532837,
      "learning_rate": 0.00017742883068638446,
      "loss": 0.5339,
      "step": 67
    },
    {
      "epoch": 1.2830188679245282,
      "grad_norm": 0.5219029188156128,
      "learning_rate": 0.00016964013367420965,
      "loss": 0.462,
      "step": 68
    },
    {
      "epoch": 1.3018867924528301,
      "grad_norm": 0.6553102135658264,
      "learning_rate": 0.00016193748801969163,
      "loss": 0.4436,
      "step": 69
    },
    {
      "epoch": 1.320754716981132,
      "grad_norm": 0.4267428517341614,
      "learning_rate": 0.00015432914190872756,
      "loss": 0.4406,
      "step": 70
    },
    {
      "epoch": 1.3396226415094339,
      "grad_norm": 1.4046710729599,
      "learning_rate": 0.00014682324254890135,
      "loss": 0.4304,
      "step": 71
    },
    {
      "epoch": 1.3584905660377358,
      "grad_norm": 3.354724407196045,
      "learning_rate": 0.00013942782744524973,
      "loss": 0.3876,
      "step": 72
    },
    {
      "epoch": 1.3773584905660377,
      "grad_norm": 0.5938930511474609,
      "learning_rate": 0.00013215081579350058,
      "loss": 0.5191,
      "step": 73
    },
    {
      "epoch": 1.3962264150943398,
      "grad_norm": 2.545119524002075,
      "learning_rate": 0.00012500000000000006,
      "loss": 0.4537,
      "step": 74
    },
    {
      "epoch": 1.4150943396226414,
      "grad_norm": 0.5195505619049072,
      "learning_rate": 0.00011798303733740801,
      "loss": 0.4658,
      "step": 75
    },
    {
      "epoch": 1.4339622641509435,
      "grad_norm": 0.4310881495475769,
      "learning_rate": 0.00011110744174509952,
      "loss": 0.425,
      "step": 76
    },
    {
      "epoch": 1.4528301886792452,
      "grad_norm": 0.8923136591911316,
      "learning_rate": 0.0001043805757830495,
      "loss": 0.5402,
      "step": 77
    },
    {
      "epoch": 1.4716981132075473,
      "grad_norm": 0.4376356601715088,
      "learning_rate": 9.780964274781984e-05,
      "loss": 0.3998,
      "step": 78
    },
    {
      "epoch": 1.490566037735849,
      "grad_norm": 0.6152018904685974,
      "learning_rate": 9.140167895908866e-05,
      "loss": 0.5527,
      "step": 79
    },
    {
      "epoch": 1.509433962264151,
      "grad_norm": 0.5422286987304688,
      "learning_rate": 8.516354622498279e-05,
      "loss": 0.3281,
      "step": 80
    },
    {
      "epoch": 1.5283018867924527,
      "grad_norm": 0.554468035697937,
      "learning_rate": 7.910192449428217e-05,
      "loss": 0.363,
      "step": 81
    },
    {
      "epoch": 1.5471698113207548,
      "grad_norm": 0.867800772190094,
      "learning_rate": 7.322330470336314e-05,
      "loss": 0.3806,
      "step": 82
    },
    {
      "epoch": 1.5660377358490565,
      "grad_norm": 0.3389556109905243,
      "learning_rate": 6.753398182554116e-05,
      "loss": 0.3745,
      "step": 83
    },
    {
      "epoch": 1.5849056603773586,
      "grad_norm": 2.383793592453003,
      "learning_rate": 6.204004813025568e-05,
      "loss": 0.5229,
      "step": 84
    },
    {
      "epoch": 1.6037735849056602,
      "grad_norm": 0.5221353769302368,
      "learning_rate": 5.6747386659315755e-05,
      "loss": 0.4913,
      "step": 85
    },
    {
      "epoch": 1.6226415094339623,
      "grad_norm": 0.5869869589805603,
      "learning_rate": 5.1661664927191235e-05,
      "loss": 0.3664,
      "step": 86
    },
    {
      "epoch": 1.641509433962264,
      "grad_norm": 0.47868937253952026,
      "learning_rate": 4.6788328852096216e-05,
      "loss": 0.5355,
      "step": 87
    },
    {
      "epoch": 1.6603773584905661,
      "grad_norm": 4.485293865203857,
      "learning_rate": 4.213259692436367e-05,
      "loss": 0.4587,
      "step": 88
    },
    {
      "epoch": 1.6792452830188678,
      "grad_norm": 0.7442128658294678,
      "learning_rate": 3.76994546183553e-05,
      "loss": 0.4872,
      "step": 89
    },
    {
      "epoch": 1.6981132075471699,
      "grad_norm": 0.9408477544784546,
      "learning_rate": 3.3493649053890325e-05,
      "loss": 0.5972,
      "step": 90
    },
    {
      "epoch": 1.7169811320754715,
      "grad_norm": 0.7044486403465271,
      "learning_rate": 2.9519683912911265e-05,
      "loss": 0.4414,
      "step": 91
    },
    {
      "epoch": 1.7358490566037736,
      "grad_norm": 0.6236746311187744,
      "learning_rate": 2.5781814616827938e-05,
      "loss": 0.5378,
      "step": 92
    },
    {
      "epoch": 1.7547169811320755,
      "grad_norm": 0.34788310527801514,
      "learning_rate": 2.2284043769706025e-05,
      "loss": 0.4554,
      "step": 93
    },
    {
      "epoch": 1.7735849056603774,
      "grad_norm": 0.27369454503059387,
      "learning_rate": 1.9030116872178316e-05,
      "loss": 0.3524,
      "step": 94
    },
    {
      "epoch": 1.7924528301886793,
      "grad_norm": 0.47575029730796814,
      "learning_rate": 1.6023518310668618e-05,
      "loss": 0.3702,
      "step": 95
    },
    {
      "epoch": 1.8113207547169812,
      "grad_norm": 0.38850137591362,
      "learning_rate": 1.3267467626223605e-05,
      "loss": 0.3092,
      "step": 96
    },
    {
      "epoch": 1.830188679245283,
      "grad_norm": 2.3478479385375977,
      "learning_rate": 1.0764916066947795e-05,
      "loss": 0.3937,
      "step": 97
    },
    {
      "epoch": 1.849056603773585,
      "grad_norm": 0.6887299418449402,
      "learning_rate": 8.51854342773295e-06,
      "loss": 0.3633,
      "step": 98
    },
    {
      "epoch": 1.8679245283018868,
      "grad_norm": 1.1624951362609863,
      "learning_rate": 6.530755180666592e-06,
      "loss": 0.6884,
      "step": 99
    },
    {
      "epoch": 1.8867924528301887,
      "grad_norm": 1.5750312805175781,
      "learning_rate": 4.803679899192393e-06,
      "loss": 0.5107,
      "step": 100
    },
    {
      "epoch": 1.9056603773584906,
      "grad_norm": 1.3390824794769287,
      "learning_rate": 3.339166978780256e-06,
      "loss": 0.4706,
      "step": 101
    },
    {
      "epoch": 1.9245283018867925,
      "grad_norm": 0.39964187145233154,
      "learning_rate": 2.1387846565474044e-06,
      "loss": 0.377,
      "step": 102
    },
    {
      "epoch": 1.9433962264150944,
      "grad_norm": 0.4270794093608856,
      "learning_rate": 1.2038183319507957e-06,
      "loss": 0.4092,
      "step": 103
    },
    {
      "epoch": 1.9622641509433962,
      "grad_norm": 0.4903929531574249,
      "learning_rate": 5.352691903491303e-07,
      "loss": 0.5199,
      "step": 104
    },
    {
      "epoch": 1.9811320754716981,
      "grad_norm": 0.6905174851417542,
      "learning_rate": 1.3385313090857886e-07,
      "loss": 0.54,
      "step": 105
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.616133451461792,
      "learning_rate": 0.0,
      "loss": 0.4532,
      "step": 106
    }
  ],
  "logging_steps": 1,
  "max_steps": 106,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 426500508672000.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
